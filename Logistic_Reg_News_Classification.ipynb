{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_Reg_News_Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADtkr9500L_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "764923eb-1fec-46df-a72d-6f020a161859"
      },
      "source": [
        "df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>headline</th>\n",
              "      <th>authors</th>\n",
              "      <th>link</th>\n",
              "      <th>short_description</th>\n",
              "      <th>date</th>\n",
              "      <th>tokenized_url</th>\n",
              "      <th>text_desc</th>\n",
              "      <th>text_desc_headline</th>\n",
              "      <th>text_desc_headline_url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CRIME</td>\n",
              "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
              "      <td>Melissa Jeltsen</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
              "      <td>She left her husband. He killed their children...</td>\n",
              "      <td>2018-05-26</td>\n",
              "      <td>texas amanda painter mass shooting us 5b081ab4...</td>\n",
              "      <td>She left her husband. He killed their children...</td>\n",
              "      <td>She left her husband. He killed their children...</td>\n",
              "      <td>She left her husband. He killed their children...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
              "      <td>Andy McDonald</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
              "      <td>Of course it has a song.</td>\n",
              "      <td>2018-05-26</td>\n",
              "      <td>will smith joins diplo and nicky jam for the o...</td>\n",
              "      <td>Of course it has a song.</td>\n",
              "      <td>Of course it has a song. Will Smith Joins Dipl...</td>\n",
              "      <td>Of course it has a song. Will Smith Joins Dipl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
              "      <td>Ron Dicker</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
              "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
              "      <td>2018-05-26</td>\n",
              "      <td>hugh grant marries us 5b09212ce4b0568a880b9a8c</td>\n",
              "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
              "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
              "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
              "      <td>Ron Dicker</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
              "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
              "      <td>2018-05-26</td>\n",
              "      <td>jim carrey adam schiff democrats us 5b0950e8e4...</td>\n",
              "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
              "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
              "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
              "      <td>Ron Dicker</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
              "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
              "      <td>2018-05-26</td>\n",
              "      <td>julianna margulies trump poop bag us 5b093ec2e...</td>\n",
              "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
              "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
              "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200848</th>\n",
              "      <td>TECH</td>\n",
              "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
              "      <td>Reuters, Reuters</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/rim-ceo-t...</td>\n",
              "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
              "      <td>2012-01-28</td>\n",
              "      <td>rim ceo thorsten heins us 5bb34b8ce4b0fa920b95...</td>\n",
              "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
              "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
              "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200849</th>\n",
              "      <td>SPORTS</td>\n",
              "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
              "      <td></td>\n",
              "      <td>https://www.huffingtonpost.com/entry/maria-sha...</td>\n",
              "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
              "      <td>2012-01-28</td>\n",
              "      <td>maria sharapova stunned victoria azarenka aust...</td>\n",
              "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
              "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
              "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200850</th>\n",
              "      <td>SPORTS</td>\n",
              "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
              "      <td></td>\n",
              "      <td>https://www.huffingtonpost.com/entry/super-bow...</td>\n",
              "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
              "      <td>2012-01-28</td>\n",
              "      <td>super bowl upsets the mos us 5bb69b1de4b097869...</td>\n",
              "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
              "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
              "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200851</th>\n",
              "      <td>SPORTS</td>\n",
              "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
              "      <td></td>\n",
              "      <td>https://www.huffingtonpost.com/entry/aldon-smi...</td>\n",
              "      <td>CORRECTION: An earlier version of this story i...</td>\n",
              "      <td>2012-01-28</td>\n",
              "      <td>aldon smith arrested dui 49ers us 5bb69b25e4b0...</td>\n",
              "      <td>CORRECTION: An earlier version of this story i...</td>\n",
              "      <td>CORRECTION: An earlier version of this story i...</td>\n",
              "      <td>CORRECTION: An earlier version of this story i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200852</th>\n",
              "      <td>SPORTS</td>\n",
              "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
              "      <td></td>\n",
              "      <td>https://www.huffingtonpost.com/entry/dwight-ho...</td>\n",
              "      <td>The five-time all-star center tore into his te...</td>\n",
              "      <td>2012-01-28</td>\n",
              "      <td>dwight howard rips teammates magic hornets us ...</td>\n",
              "      <td>The five-time all-star center tore into his te...</td>\n",
              "      <td>The five-time all-star center tore into his te...</td>\n",
              "      <td>The five-time all-star center tore into his te...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200853 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             category  ...                             text_desc_headline_url\n",
              "0               CRIME  ...  She left her husband. He killed their children...\n",
              "1       ENTERTAINMENT  ...  Of course it has a song. Will Smith Joins Dipl...\n",
              "2       ENTERTAINMENT  ...  The actor and his longtime girlfriend Anna Ebe...\n",
              "3       ENTERTAINMENT  ...  The actor gives Dems an ass-kicking for not fi...\n",
              "4       ENTERTAINMENT  ...  The \"Dietland\" actress said using the bags is ...\n",
              "...               ...  ...                                                ...\n",
              "200848           TECH  ...  Verizon Wireless and AT&T are already promotin...\n",
              "200849         SPORTS  ...  Afterward, Azarenka, more effusive with the pr...\n",
              "200850         SPORTS  ...  Leading up to Super Bowl XLVI, the most talked...\n",
              "200851         SPORTS  ...  CORRECTION: An earlier version of this story i...\n",
              "200852         SPORTS  ...  The five-time all-star center tore into his te...\n",
              "\n",
              "[200853 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa7nQA2UbVCV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "f7aaca01-33f6-4217-994b-8b3b3c120b90"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "import re\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "import pickle\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import logging\n",
        "\n",
        "\n",
        "df=pd.read_json(r\"/content/drive/My Drive/ML codes/NLP text scan/News_Category_Dataset_v2.json\",lines=True)\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_url(url:str):   \n",
        "    url=url.replace(\"https://www.huffingtonpost.com/entry/\",\"\")\n",
        "    url=re.sub(\"(\\W|_)+\",\" \",url)\n",
        "    return url\n",
        "\n",
        "df['tokenized_url']=df['link'].apply(lambda x:tokenize_url(x))\n",
        "\n",
        "#just the description\n",
        "df['text_desc'] = df['short_description']\n",
        "\n",
        "#description + headline\n",
        "df['text_desc_headline'] = df['short_description'] + ' '+ df['headline']\n",
        "\n",
        "#description + headline + tokenized url\n",
        "df['text_desc_headline_url'] = df['short_description'] + ' '+ df['headline']+\" \" + df['tokenized_url']\n",
        "\n",
        "def _reciprocal_rank(true_labels: list, machine_preds: list):\n",
        "    \"\"\"Compute the reciprocal rank at cutoff k\"\"\"\n",
        "    \n",
        "    # add index to list only if machine predicted label exists in true labels\n",
        "    tp_pos_list = [(idx + 1) for idx, r in enumerate(machine_preds) if r in true_labels]\n",
        "\n",
        "    rr = 0\n",
        "    if len(tp_pos_list) > 0:\n",
        "        # for RR we need position of first correct item\n",
        "        first_pos_list = tp_pos_list[0]\n",
        "        \n",
        "        # rr = 1/rank\n",
        "        rr = 1 / float(first_pos_list)\n",
        "\n",
        "    return rr\n",
        "\n",
        "def compute_mrr_at_k(items:list):\n",
        "    \"\"\"Compute the MRR (average RR) at cutoff k\"\"\"\n",
        "    rr_total = 0\n",
        "    \n",
        "    for item in items:   \n",
        "        rr_at_k = _reciprocal_rank(item[0],item[1])\n",
        "        rr_total = rr_total + rr_at_k\n",
        "        mrr = rr_total / 1/float(len(items))\n",
        "\n",
        "    return mrr\n",
        "\n",
        "def collect_preds(Y_test,Y_preds):\n",
        "    \"\"\"Collect all predictions and ground truth\"\"\"\n",
        "    \n",
        "    pred_gold_list=[[[Y_test[idx]],pred] for idx,pred in enumerate(Y_preds)]\n",
        "    return pred_gold_list\n",
        "             \n",
        "def compute_accuracy(eval_items:list):\n",
        "    correct=0\n",
        "    total=0\n",
        "    \n",
        "    for item in eval_items:\n",
        "        true_pred=item[0]\n",
        "        machine_pred=set(item[1])\n",
        "        \n",
        "        for cat in true_pred:\n",
        "            if cat in machine_pred:\n",
        "                correct+=1\n",
        "                break\n",
        "    print(\"\\n\\n\\ncorrect = \",correct)\n",
        "    accuracy=correct/float(len(eval_items))\n",
        "    return accuracy\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "def extract_features(df,field,training_data,testing_data,type=\"binary\"):\n",
        "    \"\"\"Extract features using different methods\"\"\"\n",
        "    \n",
        "    logging.info(\"Extracting features and creating vocabulary...\")\n",
        "    \n",
        "    if \"binary\" in type:\n",
        "        \n",
        "        # BINARY FEATURE REPRESENTATION\n",
        "        cv= CountVectorizer(binary=True, max_df=0.95)\n",
        "        cv.fit_transform(training_data[field].values)\n",
        "        train_feature_set=cv.transform(training_data[field].values)\n",
        "        test_feature_set=cv.transform(testing_data[field].values)\n",
        "        # print(\"\\n\\n\\n  train_feature_set = \", train_feature_set)\n",
        "        return train_feature_set,test_feature_set,cv\n",
        "  \n",
        "    elif \"counts\" in type:\n",
        "        \n",
        "        # COUNT BASED FEATURE REPRESENTATION\n",
        "        cv= CountVectorizer(binary=False, max_df=0.95)\n",
        "        cv.fit_transform(training_data[field].values)\n",
        "        \n",
        "        train_feature_set=cv.transform(training_data[field].values)\n",
        "        test_feature_set=cv.transform(testing_data[field].values)\n",
        "        \n",
        "        return train_feature_set,test_feature_set,cv\n",
        "    \n",
        "    else:    \n",
        "        \n",
        "        # TF-IDF BASED FEATURE REPRESENTATION\n",
        "        tfidf_vectorizer=TfidfVectorizer(use_idf=True, max_df=0.95)\n",
        "        tfidf_vectorizer.fit_transform(training_data[field].values)\n",
        "        \n",
        "        train_feature_set=tfidf_vectorizer.transform(training_data[field].values)\n",
        "        test_feature_set=tfidf_vectorizer.transform(testing_data[field].values)\n",
        "        \n",
        "        return train_feature_set,test_feature_set,tfidf_vectorizer\n",
        "\n",
        "def get_top_k_predictions(model,X_test,k):\n",
        "    \n",
        "    # get probabilities instead of predicted labels, since we want to collect top 3\n",
        "    probs = model.predict_proba(X_test)\n",
        "\n",
        "    # GET TOP K PREDICTIONS BY PROB - note these are just index\n",
        "    best_n = np.argsort(probs, axis=1)[:,-k:]\n",
        "    \n",
        "    # GET CATEGORY OF PREDICTIONS\n",
        "    preds=[[model.classes_[predicted_cat] for predicted_cat in prediction] for prediction in best_n]\n",
        "    \n",
        "    preds=[ item[::-1] for item in preds]\n",
        "    \n",
        "    return preds\n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_model(df,field=\"text_desc\",feature_rep=\"binary\",top_k=3):\n",
        "    \n",
        "    logging.info(\"Starting model training...\")\n",
        "    \n",
        "    # GET A TRAIN TEST SPLIT (set seed for consistent results)\n",
        "    training_data, testing_data = train_test_split(df,random_state = 2000,)\n",
        "    #print(\"\\n\\n\\n training_data = \",training_data)\n",
        "    # GET LABELS\n",
        "    Y_train=training_data['category'].values\n",
        "    Y_test=testing_data['category'].values\n",
        "   \n",
        "    # GET FEATURES\n",
        "    X_train,X_test,feature_transformer=extract_features(df,field,training_data,testing_data,type=feature_rep)\n",
        "    # print(\"\\n\\n\\\\nY X train shape = \",(X_train))\n",
        "    # print(\"\\n\\n\\\\nY X test shape = \",(X_test))\n",
        "    \n",
        "    # INIT LOGISTIC REGRESSION CLASSIFIER\n",
        "    logging.info(\"Training a Logistic Regression Model...\")\n",
        "    scikit_log_reg = LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000)\n",
        "    model=scikit_log_reg.fit(X_train,Y_train)\n",
        "    \n",
        "    # GET TOP K PREDICTIONS\n",
        "    preds=get_top_k_predictions(model,X_test,top_k)\n",
        "    \n",
        "    # GET PREDICTED VALUES AND GROUND TRUTH INTO A LIST OF LISTS - for ease of evaluation\n",
        "    eval_items=collect_preds(Y_test,preds)\n",
        "    \n",
        "    # GET EVALUATION NUMBERS ON TEST SET -- HOW DID WE DO?\n",
        "    logging.info(\"Starting evaluation...\")\n",
        "    accuracy=compute_accuracy(eval_items)\n",
        "    mrr_at_k=compute_mrr_at_k(eval_items)\n",
        "    # print(\"\\n\\n\\nEVAL ITEMS = \",eval_items)\n",
        "    \n",
        "    logging.info(\"Done training and evaluation.\")\n",
        "    #s=LogisticRegression.score(Y_test, preds,y=None)\n",
        "    #print(\"\\n\\n\\ns =\",s)\n",
        "    \n",
        "    return model,feature_transformer,accuracy,mrr_at_k\n",
        "\n",
        "\n",
        "field='text_desc'\n",
        "feature_rep='binary'\n",
        "top_k=3\n",
        "\n",
        "model,transformer,accuracy,mrr_at_k=train_model(df,field=field,feature_rep=feature_rep,top_k=top_k)\n",
        "\n",
        "\n",
        "model_path=\"/content/drive/My Drive/ML codes/NLP text scan/model.pkl\"\n",
        "transformer_path=\"/content/drive/My Drive/ML codes/NLP text scan/transformer.pkl\"\n",
        "\n",
        "# pickle.dump(model,open(model_path, 'wb'))\n",
        "# pickle.dump(transformer,open(transformer_path,'wb'))\n",
        "\n",
        "\n",
        "# loaded_model = pickle.load(open(model_path, 'rb'))\n",
        "# loaded_transformer = pickle.load(open(transformer_path, 'rb'))\n",
        "\n",
        "# test_features=transformer.transform([\"True Thompson makes an adorable cameo in Khloe Kardashian's new makeup tutorial video\"])\n",
        "\n",
        "print(\"\\nAccuracy={0}; MRR={1}\".format(accuracy,mrr_at_k))\n",
        "# print(get_top_k_predictions(loaded_model,test_features,2))\n",
        "\n",
        "\n",
        "#print(sklearn.metrics.confusion_matrix(test_features, pre, labels=None, sample_weight=None, normalize=None))\n",
        "#print(\"acc=\",sklearn.metrics.accuracy_score(test_features, pre, normalize=True, sample_weight=None))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-27 12:30:45,682 : INFO : Starting model training...\n",
            "2020-04-27 12:30:45,949 : INFO : Extracting features and creating vocabulary...\n",
            "2020-04-27 12:30:52,648 : INFO : Training a Logistic Regression Model...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-04-27 12:45:07,184 : INFO : Starting evaluation...\n",
            "2020-04-27 12:45:07,308 : INFO : Done training and evaluation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "correct =  30820\n",
            "\n",
            "Accuracy=0.6137730513402637; MRR=0.5026121533171167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T47h50eGgOgy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58a7144f-1474-426a-ac52-bf30542d4eaf"
      },
      "source": [
        "test_features=transformer.transform([\"George Aref Nader, who was a key witness in special counsel Robert Mueller's Russia investigation, faces new charges of transporting a minor with intent to engage in criminal sexual activity and child pornography\"])\n",
        "pre=(get_top_k_predictions(model,test_features,2))\n",
        "pre"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['POLITICS', 'CRIME']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX3vUDXb0bS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}